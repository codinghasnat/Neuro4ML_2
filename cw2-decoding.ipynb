{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFnP5PWrctES"
      },
      "source": [
        "# Neuro4ML Coursework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this week's coursework, you will use a data set of spike trains recorded from monkey motor cortex while it was doing a task involving moving a pointer on a screen. The aim of this coursework is to decode the recorded velocity of the pointer from the neural data using a network of leaky integrate-and-fire neurons that take the recorded spikes as input and give sequences of velocities as outputs. You will train these networks using surrogate gradient descent. If you haven't already looked at it, a great starting point is Friedemann Zenke's [SPyTorch tutorial notebook 1](https://github.com/fzenke/spytorch/blob/main/notebooks/SpyTorchTutorial1.ipynb) (and the rest are worth looking at too).\n",
        "\n",
        "In this coursework, we are following the general approach of the article [\"Machine learning for neural decoding\" (Glaser et al. 2020)](https://doi.org/10.1523/ENEURO.0506-19.2020), but using a spiking neural network decoder instead of the statistical and artificial neural network models used in that paper. You can also have a look at the [GitHub repository for the paper](https://github.com/KordingLab/Neural_Decoding). In case you're interested, the data were originally recorded for the paper [\"Population coding of conditional probability distributions in dorsal premotor cortex\" (Glaser et al. 2018)](https://doi.org/10.1038/s41467-018-04062-6), but you do not need to read this paper to understand this coursework.\n",
        "\n",
        "The general setup is illustrated in this figure:\n",
        "\n",
        "![Cartoon of decoder setup](cartoon.png)\n",
        "\n",
        "You are given an array of ``num_neurons`` spike trains in a variable ``spike_trains``. This variable is a Python list of numpy arrays, each numpy array has a different length and is the recorded times (in seconds) that the corresponding neuron fired a spike. You also have two additional arrays ``vel`` and ``vel_times`` where ``vel`` has shape ``(num_time_points, 2)`` and ``vel_times`` has has shape ``(num_time_points)``. The second axis of ``vel`` has length 2 corresponding to the x and y-components of the recorded velocity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugUY4mlbdQ2Z"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaymEeXAc6Dp"
      },
      "source": [
        "This section has some basics to get you started.\n",
        "\n",
        "Let's start by importing some libraries you can make use of. You can solve all the task only using the imports below, but you are welcome to add your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ags-Wad--GP"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as tnnf\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "ms = 1e-3 # use this constant so you can write e.g. 1*ms for a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GHpT0oVc9pw"
      },
      "source": [
        "You already have a copy of the raw data, but for your information, here is where the original can be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request, zipfile, os\n",
        "filename = 's1_data_raw.mat'\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve('https://www.dropbox.com/sh/n4924ipcfjqc0t6/AACPWjxDKPEzQiXKUUFriFkJa?dl=1', 'data.zip')\n",
        "    with zipfile.ZipFile('data.zip') as z:\n",
        "        z.extract(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTPju6FQdb8i"
      },
      "source": [
        "## Task 1: Load and plot the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycf87WWXdGQx"
      },
      "source": [
        "The code below first loads the raw data, which is stored as a Matlab file, and then extracts the three arrays ``spike_times``, ``vel`` and ``vel_times``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FwkbNQnFC_AN"
      },
      "outputs": [],
      "source": [
        "# Load the raw data\n",
        "data = io.loadmat('s1_data_raw.mat') # a matlab file!\n",
        "spike_times = [st[:, 0] for st in data['spike_times'].ravel()] # a list of arrays of spike times in seconds, one for each neuron, spike times in seconds\n",
        "vel = data['vels'] # velocity data shape (num_time_points, 2) for (x, y) coordinates\n",
        "vel_times = data['vel_times'].squeeze() # times the velocities were recorded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWyklhNdJZi"
      },
      "source": [
        "### Task 1A: Preprocess and compute basic statistics\n",
        "\n",
        "In this task, you will preprocess the data, extract some basic statistics from it.\n",
        "\n",
        "1. Whiten the recorded velocities (i.e. transform them so that their mean is 0 and standard deviation is 1).\n",
        "2. Compute and print out the number of neurons and number of spikes recorded.\n",
        "3. Compute and print out the duration of the experiment in seconds and/or minutes.\n",
        "4. Compute and print out the sampling rate at which spikes were recorded (or find the information in the corresponding paper).\n",
        "5. Compute and print out the sampling rate at which velocities were recorded (or find the information in the corresponding paper).\n",
        "\n",
        "Note that the spikes and velocities were recorded with different equipment and so they have different sampling rates. Think about how you can estimate these sampling rates from the recorded data (or look it up in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1B: Plot the data\n",
        "\n",
        "In this task, you will plot the data to get a feeling for what it is like (an important step in any modelling).\n",
        "\n",
        "1. Plot the spike times as a raster plot (black dots at x-coordinates the time of the spike, and y-coordinates the index of the neuron). Plot this both for the whole data set and for the period from 1000 to 1010 seconds.\n",
        "2. Plot the x- and y-coordinates of the velocities. Plot this both for the whole data set and for the same period as above for the spikes.\n",
        "3. Compute the mean firing rate (number of spikes per second) for each neuron and display as a bar chart.\n",
        "4. Plot the velocities as a curve in (x, y) space, emphasising the part of the velocity curve for the period above.\n",
        "\n",
        "You can use the template below to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# # Plot all spikes\n",
        "# ax = plt.subplot(231)\n",
        "# # ...\n",
        "# plt.ylabel('Neuron index')\n",
        "# # Plot all x- and y- components of the velocities\n",
        "# plt.subplot(234, sharex=ax)\n",
        "# # ...\n",
        "# plt.xlabel('Time (s)')\n",
        "# plt.ylabel('velocity')\n",
        "# plt.legend(loc='best')\n",
        "# # Plot spikes at times t=1000 to t=1010\n",
        "# ax = plt.subplot(232)\n",
        "# # ...\n",
        "# # Plot velocities at times t=1000 to t=1010\n",
        "# plt.subplot(235, sharex=ax)\n",
        "# plt.xlabel('Time (s)')\n",
        "\n",
        "# # Compute firing rates for each neuron and plot as a histogram\n",
        "# plt.subplot(233)\n",
        "# firing_rates = ...\n",
        "# plt.barh(range(len(spike_times)), firing_rates, height=1)\n",
        "# plt.xlabel('Firing rate (sp/s)')\n",
        "# plt.ylabel('Neuron index')\n",
        "\n",
        "# # Plot all velocities as points in (x, y)-plane as a continuous curve\n",
        "# # Emphasise the region from t=1000 to t=1010 with a different colour\n",
        "# plt.subplot(236)\n",
        "# # ...\n",
        "# plt.xlabel('X velocity')\n",
        "# plt.ylabel('Y velocity')\n",
        "\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKYHCPj0dmih"
      },
      "source": [
        "## Task 2: Divide data into test/train and batches\n",
        "\n",
        "1. As in any machine learning task, divide your data up into a non-overlapping training set, testing set and (optionally) validation set.\n",
        "2. Write a generator function (see below) ``batched_data`` that iterates over your data in randomly ordered segments of a given length, returning it in batches. The function should have arguments that determine the range of data to use, the simulation time step that will be used, the length (in seconds) of each batch, and the batch size (you may add additional arguments if you wish). The function should return a pair of arrays ``(x, y)``. The array ``x`` has shape ``(batch_size, num_neurons, num_time_points)`` containing the spike times as an array where a zero indicates no spike and 1 indicates a spike. Here ``num_time_points`` is the number of time points in the batch measured at the sampling rate of the simulation time step, not the number of time points in the data as a whole, nor at the spike or velocity sampling rate. The array ``y`` has shape ``(batch_size, 2, num_time_points)`` containing the velocities at the same time points as the spikes. You will need to use some sort of interpolation to get the velocities at these times.\n",
        "3. Plot a sample of spike times and velocities for a random batch of length 1 second and ``batch_size=4``.\n",
        "\n",
        "**Note on generation functions**\n",
        "\n",
        "Generator functions are an advanced feature of Python that makes it easy to iterate over complicated datasets. The general syntax is just a standard function that uses the keyword ``yield`` instead of ``return`` to return data, which allows it to return data multiple times. You can iterate over the values returned by a generator function instead of just calling it. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gimme_some_multiples(n):\n",
        "    yield n*1\n",
        "    yield n*2\n",
        "    yield n*3\n",
        "\n",
        "for x in gimme_some_multiples(3):\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And another:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gimme_some_more_multiples(n, how_many):\n",
        "    for i in range(how_many):\n",
        "        yield n*(i+1)\n",
        "\n",
        "for x in gimme_some_more_multiples(5, 4):\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the following template but you may want to define some additional helper functions to simplify your code and that you can re-use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Template\n",
        "\n",
        "# # Set up training / validation / testing ranges\n",
        "# # ...\n",
        "\n",
        "# # Generator function that yields batches\n",
        "# def batched_data(range_to_use, dt=1e-3, length=1, batch_size=64):\n",
        "#   pass # ...\n",
        "#   for batch_idx in range(num_batches):\n",
        "#     x = torch.zeros((batch_size, num_neurons, num_time_points))\n",
        "#     y = torch.zeros((batch_size, 2, num_time_points))\n",
        "#     for b in range(batch_size):\n",
        "#       pass # ...\n",
        "#     yield x, y\n",
        "\n",
        "# # Plot a sample of data\n",
        "\n",
        "# x, y = next(batched_data(...)) # this just gets the first item of an iterable\n",
        "\n",
        "# plt.figure(figsize=(12, 5))\n",
        "# for b in range(4):\n",
        "#   # Plot spikes for this batch index\n",
        "#   ax = plt.subplot(2, 4, b+1)\n",
        "#   # ...\n",
        "#   plt.ylabel('Neuron index')\n",
        "#   plt.title(f'Batch index {b}')\n",
        "#   # Plot velocities for this batch index\n",
        "#   plt.subplot(2, 4, b+5, sharex=ax)\n",
        "#   plt.xlabel('Time index')\n",
        "#   plt.ylabel('velocity')\n",
        "#   if b==0:\n",
        "#     plt.legend(loc='best')\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU22zpXQT0pq"
      },
      "source": [
        "## Surrogate gradient descent spike function\n",
        "\n",
        "Below is the code for the surrogate gradient descent function from lectures. You can use it as is, although note that there is a hyperparameter (scale) that you can experiment with if you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJpnQEElTz4i"
      },
      "outputs": [],
      "source": [
        "class SurrogateHeaviside(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements\n",
        "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid\n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "\n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which\n",
        "        we need to later backpropagate our error signals. To achieve this we use the\n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the\n",
        "        surrogate gradient of the loss with respect to the input.\n",
        "        Here we use the normalized negative part of a fast sigmoid\n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrogateHeaviside.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "\n",
        "# here we overwrite our naive spike function by the \"SurrogateHeaviside\" nonlinearity which implements a surrogate gradient\n",
        "surrogate_heaviside  = SurrogateHeaviside.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McTDI4llQjQa"
      },
      "source": [
        "## Task 3: Simulation code\n",
        "\n",
        "Write modular code to simulate a layer of leaky integrate-and-fire spiking neurons compatible with autodifferentiation with PyTorch. You can either write this as a function or class. The code should accept an input batch of spikes ``x`` of shape ``(batch_size, num_input_neurons, num_time_points)`` and values 0 or 1 (as in the ``batched_data`` generator function above). The code should have the option to produce either spiking or non-spiking output. In both cases, the output should be an array ``y`` of shape ``(batch_size, num_output_neurons, num_time_points)``. In the case of spiking output, the values of ``y`` should be 0s and 1s, and in the case of non-spiking output they should be the membrane potential values. You may also want to write an additional class to handle multiple layers of spiking neural networks for subsequent tasks.\n",
        "\n",
        "Your code should include initialisation of the weight matrices, and add additional hyperparameters for this initialisation. You may also want to make the time constants of your neurons into hyperparameters. I used ``tau=50*ms`` for spiking neurons and ``tau=500*ms`` for non-spiking neurons and it worked OK, but I didn't do an extensive hyperparameter search.\n",
        "\n",
        "I would recommend approaching this and the following sections as follows:\n",
        "\n",
        "1. Write simulation code for a single layer, non-spiking neural network first. This code is simpler and will train fast (under 3 minutes on Colab). Attempt as much of the remaining tasks as possible using only this.\n",
        "2. Add the ability for spiking and test that your code produces reasonable output but don't try to train it yet.\n",
        "3. Add the ability to plot spiking hidden layers and try to get a reasonable initialisation of the network.\n",
        "4. Start training the spiking neural network. Your final run will probably take a long time to train but you should build up to that by seeing how well the training curves improve for fewer epochs, batch sizes, etc.\n",
        "\n",
        "You can use the template below if you want to use the class-based approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class SNNLayer(nn.Module):\n",
        "#   def __init__(self, n_in, n_out, spiking=True):\n",
        "#     super(SNNLayer, self).__init__()\n",
        "#     self.n_in = n_in\n",
        "#     self.n_out = n_out\n",
        "#     self.spiking = spiking\n",
        "#     # Store weights as a trainable parameter\n",
        "#     self.w = nn.Parameter(torch.ones((n_in, n_out)))\n",
        "#     # ...\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     batch_size, num_neurons, num_time_points = x.shape\n",
        "#     # ...\n",
        "#     return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluation functions\n",
        "\n",
        "Write code that takes a network and testing range as input and returns the mean loss over the testing data. The loss is the mean squared error of the output of the network compared to the target data. You may also find it helpful to compute the null loss, which is the loss you would get if you just output all zeros. You should be able to do better than this!\n",
        "\n",
        "Also write code that plots some of the internal outputs of the network, for example to show you the spikes produced by hidden layers, calculate their firing rates, etc.\n",
        "\n",
        "Initialise a network with one hidden layer of 100 spiking neurons and one output layer of 2 non-spiking neurons. Run this on a random sample of the data of length 1 and plot the input spikes, hidden layer spikes, output x- and y-velocities, and the data x- and y-velocities. For each spiking layer compute the firing rates of each neuron and plot them as a histogram.\n",
        "\n",
        "You can use this to initialise your networks in a reasonable state. Hidden layers should fire spikes at rates that are not too low and not too high. I aimed for an average firing rate in the range 20-100 and it worked well, but you can experiment with other options. The output layer should give values that are roughly in the right range as the data (i.e. it shouldn't go to +/- 100 if the data is going to only +/- 4). If you look at the spike trains of your hidden layer and all the neurons at initialisation are doing exactly the same thing then it's probably not going to learn very well, so try out some different weight initialisations to see if you can do better.\n",
        "\n",
        "Print the value of the loss (and optionally the null loss) for your untrained network, to give you an idea of the baseline.\n",
        "\n",
        "You may want to wrap your evaluation code in a ``with`` statement like below to stop PyTorch from computing gradients when evaluating (unnecessary and expensive):\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    ...\n",
        "    # whatever you do here won't compute any gradients\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Training\n",
        "\n",
        "Start with a single non-spiking output layer (i.e. spikes connected directly via a weight matrix to two non-spiking LIF neurons as output). Find a good initialisation for this network that gives outputs roughly in the right range.\n",
        "\n",
        "Select an optimisation algorithm, learning rate, etc. and train your network.\n",
        "\n",
        "Has your algorithm converged?\n",
        "\n",
        "You should be able to do better than the null loss (but you don't need to do hugely better). I get a null loss of around 0.8 and a trained loss of around 0.6.\n",
        "\n",
        "At the end of training, plot your loss curves for training and test/validation data and print out your testing loss. Plot the output of your model and compare to the target data for 8 randomly sampled time windows of length 1. You may notice that your network matches the data better in the second half of the window than the first half, because the network always starts at a value zero even if the data doesn't. We'll look into this more in the next task. Don't worry too much if the fits don't look great at this stage. If you get a mean squared error of around .75 of the null MSE then you're doing fine.\n",
        "\n",
        "You may want to use the following code as a starting point (it worked well enough for me but you can probably do better). On my desktop with CPU only, this took about two minutes to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Training parameters\n",
        "# lr = 0.001 # learning rate\n",
        "# num_epochs = 10\n",
        "# max_num_batches = 40\n",
        "# length = 1\n",
        "# batch_size = 32 # small batches worked better for me for some reason\n",
        "\n",
        "# # Optimiser and loss function\n",
        "# optimizer = torch.optim.Adam(..., lr=lr) # what should the first argument be?\n",
        "\n",
        "# # Training\n",
        "# loss_hist = []\n",
        "# val_loss_hist = []\n",
        "# with tqdm(total=num_epochs*max_num_batches) as pbar:\n",
        "#   last_epoch_loss = val_loss = null_val_loss = None\n",
        "#   for epoch in range(num_epochs):\n",
        "#     local_loss = []\n",
        "#     for x, y in batched_data(...):\n",
        "#       # Run the network\n",
        "#       y_out = net(x)\n",
        "#       # Compute a loss\n",
        "#       loss = mse(y_out, y)\n",
        "#       local_loss.append(loss.item())\n",
        "#       # Update gradients\n",
        "#       optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       pbar.update(1)\n",
        "#       pbar.set_postfix(epoch=epoch, last_epoch_loss=last_epoch_loss, loss=loss.item(), val_loss=val_loss, null_val_loss=null_val_loss)\n",
        "#     last_epoch_loss = np.mean(local_loss)\n",
        "#     val_loss, null_val_loss = evaluate_network(net, ...)\n",
        "#     pbar.set_postfix(epoch=epoch, last_epoch_loss=last_epoch_loss, loss=loss.item(), val_loss=val_loss, null_val_loss=null_val_loss)\n",
        "#     loss_hist.append(last_epoch_loss)\n",
        "#     val_loss_hist.append(val_loss)\n",
        "\n",
        "# # Plot the loss function over time\n",
        "# plt.semilogy(loss_hist, label='Testing loss')\n",
        "# plt.semilogy(val_loss_hist, label='Validation loss')\n",
        "# plt.axhline(null_val_loss, ls='--', c='r', label='Null model loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('MSE')\n",
        "# plt.legend(loc='best')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# testing_loss, null_testing_loss = evaluate_network(net, testing_range, length=length, batch_size=batch_size)\n",
        "# print(f'{testing_loss=}, {null_testing_loss=}')\n",
        "\n",
        "# # Plot trained output\n",
        "# plt.figure(figsize=(16, 6))\n",
        "# with torch.no_grad():\n",
        "#   for x, y in batched_data(..., batch_size=8, max_num_batches=1):\n",
        "#     for b in range(8):\n",
        "#       plt.subplot(2, 4, b+1)\n",
        "#       y_out = net(x)\n",
        "#       plt.plot(y_out[b, 0, :], ':C0', label='x_out')\n",
        "#       plt.plot(y_out[b, 1, :], ':C1', label='y_out')\n",
        "#       plt.plot(y[b, 0, :], '--C0', label='x')\n",
        "#       plt.plot(y[b, 1, :], '--C1', label='y')\n",
        "#       # Plot a smoothed version as well\n",
        "#       plt.plot(savgol_filter(y_out[b, 0, :], 151, 3), '-C0', label='x_out (smooth)')\n",
        "#       plt.plot(savgol_filter(y_out[b, 1, :], 151, 3), '-C1', label='y_out (smooth)')\n",
        "#       plt.ylim(-5, 5)\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6: Longer length decoding\n",
        "\n",
        "Your code above probably doesn't look great when plotted. That's partly because the outputs start at 0 but the data doesn't necessarily have to, so it takes a while for them to get in sync, and partly because on some intervals it will just do badly. To fix this, and to extend the fit to a longer range of data, in this task we only use the final timestep of each segment and compare to the data. Take a 15 second segment of testing data, and sample every 0.2 seconds to get 75 data points. For each data point, take a 1 second segment of time before this data point (these will be overlapping), run your simulation for that one second, and use the final time point of the simulated output as your prediction. Plot this compared to the real data for 8 different segments of 15 seconds.\n",
        "\n",
        "This should look like a reasonable fit to the data. Congratulations, you have used raw spiking output of neurons recorded from a monkey's brain to predict what it was doing on a computer screen it was interacting with. That's a brain machine interface right here.\n",
        "\n",
        "You can use the template below to get you started.\n",
        "\n",
        "Your results might look something like this:\n",
        "\n",
        "![Fits](fits.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def decoding_plot(decoding_range, dt_decoding=0.2, decoding_length=15, length=1, dt=1e-3, figdims=(4, 2), ...):\n",
        "#     w_intervals = np.arange(decoding_range[0]+length, decoding_range[0]+length+decoding_length, dt_decoding)\n",
        "#     batch_size_single = len(w_intervals)-1\n",
        "#     num_time_points = int(np.round(length/dt))\n",
        "#     nfx, nfy = figdims\n",
        "#     nf = nfx*nfy\n",
        "#     batch_size = nf*batch_size_single\n",
        "#     nrows = nfy*2\n",
        "#     ncols = nfx\n",
        "#     with torch.no_grad():\n",
        "#         x = torch.zeros((batch_size, num_neurons, num_time_points))\n",
        "#         y = torch.zeros((batch_size, 2, num_time_points))\n",
        "#         T = []\n",
        "#         for b in range(batch_size):\n",
        "#            w_start = decoding_range[0]+dt_decoding*b\n",
        "#            w_end = w_start+length\n",
        "#            T.append(w_end)\n",
        "#            # ... (copy data to x, y)\n",
        "#         T = np.array(T)\n",
        "#         y_out = ...\n",
        "#         mean_mse = mse(y, y_out)\n",
        "#         plt.figure(figsize=(ncols*3, nrows*2))\n",
        "#         for nf_i in range(nf):\n",
        "#             sp_x = nf_i % nfx\n",
        "#             sp_y = nf_i // nfx\n",
        "#             for i in range(2):\n",
        "#                 plt.subplot(nrows, ncols, sp_x+1+2*ncols*sp_y+i*ncols)\n",
        "#                 # ...\n",
        "#                 plt.ylim(-4, 4)\n",
        "#                 if sp_x==0:\n",
        "#                     plt.ylabel('Velocity')\n",
        "#                 if 2*sp_y+i==nrows-1:\n",
        "#                     plt.xlabel('Time (s)')\n",
        "#                 if nf_i==0:\n",
        "#                     plt.legend(loc='best')\n",
        "#         plt.suptitle(f'{mean_mse=:.3f}')\n",
        "#         plt.tight_layout()\n",
        "\n",
        "# decoding_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 7: Comparing spiking and non-spiking\n",
        "\n",
        "Now try training your network with at least one spiking hidden layer. Compare your results to the non-spiking version. Note that training times with a spiking hidden layer are likely to be much longer. My training time went up from 2 minutes to 30 minutes (with CPU only).\n",
        "\n",
        "With the spiking layer do you do worse, as well, or better? (There is no correct answer here, but if you can do much better let me know we might be able to write a paper on this.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "neuro4ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
